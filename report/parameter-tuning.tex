\section{Parameter tuning}
\label{sec:parameter-tuning}

In order to find the best set of parameters for the ALNS and Tabu Search, different parameter combinations was tried (see section \ref{sec:parameter-tabu} and \ref{sec:parameter-alns}). Each parameter combination was tried 3 times using different random initializations.

Because the problems aren't equally difficult and because the objective value isn't normalized, the objective value for each dataset can't be directly compared. To accommodate this the best objective value for each dataset is used to normalize the objective, this score is called the \textit{gap}:
\begin{equation}
\tilde{z}_i = \frac{z_i - z^*}{z^*}
\end{equation}
Here $z_i$ is the objective value and $z^*$ is the best objective value for the dataset.

Because one wishes to avoid overfitting of the parameters, a subset of the entire dataset is chosen for parameter optimization, this is called the training dataset. As there do not appear to be any pattern in the dataset id, all odd dataset are chosen for parameter optimization.

\begin{table}[H]
\centering
\begin{tabular}{l|rrrrrrr}
 dataset id &   1 &   3 &   5 &    7 &   9 &   11 &   13 \\
\hline
 Tabu   &  35 & 706 & 896 & 1390 & 765 &   36 &  794 \\
 ALNS   &  24 & 211 & 761 &  211 & 200 &    5 &  166 \\
 both   &  24 & 211 & 761 &  211 & 200 &    5 &  166 \\
\end{tabular}
\caption{Best objective value for each training dataset}
\end{table}

\subsection{Tabu}
\label{sec:parameter-tabu}

A grid search over all 4 parameters (see table \ref{table:tabu-search-summary}) is performed. This is $108$ $(\mu, \sigma)$ pairs, which is too much data to visualize. Thus tables with two parameters and the remaining fixed to the best parameters are shown instead.

\begin{table}[H]
\centering
\centerline{\begin{tabular}{rr|ccc}
 &  & \multicolumn{3}{c}{\texttt{intensification}}\\
 &  & 2 & 10 & $\infty$\\
\hline
\multirow{3}{*}{\texttt{diversification}} & 0 & (4.90, 0.52) & (5.36, 0.37) & (5.53, 0.71)\\
 & 1 & (5.07, 0.97) & (5.34, 0.17) & (5.29, 0.63)\\
 & 5 & (5.19, 0.41) & (4.14, 0.12) & (5.06, 0.77)\\
\end{tabular}}
\caption{Shows $(\mu, \sigma)$ with \texttt{allow\_swap=dynamic} and \texttt{tabu\_limit=40} fixed}
\end{table}

The choice of diversification and intensification appears to be somewhat important. There don't appear to be any linear trend, it is the combination $diversification=5$ and $intensification=10$ that yields good result. This makes sense since intensification reverts the diversification, thus they need to fit together. $diversification$ is on the edge of the grid search, given more time one should investigate this parameter direction further.

\begin{table}[H]
\centering
\centerline{\begin{tabular}{rr|cccc}
 &  & \multicolumn{4}{c}{\texttt{tabu\_limit}}\\
 &  & 10 & 20 & 40 & $\infty$ \\
\hline
\multirow{3}{*}{\texttt{allow\_swap}} & never & (5.53, 0.70) & (5.91, 0.89) & (6.52, 0.60) & (5.93, 0.43)\\
 & always & (9.12, 0.18) & (8.77, 0.26) & (9.16, 0.87) & (8.82, 0.44)\\
 & dynamic & (5.52, 0.69) & (5.10, 0.59) & (4.14, 0.12) & (5.45, 0.27)\\
\end{tabular}}
\caption{Shows $(\mu, \sigma)$ with \texttt{diversification=5} and \texttt{intensification=10} fixed}
\end{table}

Using the dynamic swap neighborhood generally outperforms the other options, though for some tabu limits only slightly. However for the correct parameters, the results shows that having a dynamic neighborhood can greatly outperform a fixed neighborhood. This idea is something that could be applied to other problems as well.

The best parameters are chosen solely based on the $\mu$ values. This is because the $\sigma$ values don't vary too much, but the chosen parameters also turns out to have the best $\sigma$ within the shown subset.

\begin{table}[H]
\centering
\begin{tabular}{r|cc}
parameter & search space & value \\ \hline
allow swap & $\{\text{never}, \text{always}, \text{dynamic}\}$ & dynamic \\
tabu limit & $\{10, 20, 40, \infty\}$ & 40 \\
intensification & $\{2, 10, \infty\}$ & 10 \\
diversification & $\{0, 1, 5\}$ & 5
\end{tabular}
\caption{Best Tabu search parameters with $\mu = 4.139$ and $\sigma = 0.122$}
\label{table:tabu-search-summary}
\end{table}

\subsection{ALNS}
\label{sec:parameter-alns}

\begin{table}[H]
\centering
\centerline{\begin{tabular}{rr|ccc}
 &  & \multicolumn{3}{c}{\texttt{remove}}\\
 &  & 1 & 3 & 5\\
\hline
\multirow{3}{*}{\texttt{update\_lambda}} & 0.9 & (0.62, 0.04) & (0.58, 0.02) & (0.99, 0.04)\\
 & 0.95 & (0.46, 0.13) & (1.09, 0.05) & (1.22, 0.05)\\
 & 0.99 & (0.35, 0.06) & (1.86, 0.04) & (1.72, 0.15)\\
\end{tabular}}
\caption{Shows $(\mu, \sigma)$ with \texttt{w\_global=10} and \texttt{w\_current=10} fixed}
\end{table}

$remove=1$ is not unreasonable as it allows for fine tuning by only inviting minor changes. However a swap operation would requires at least $remove=2$, given more project time this choice should be explored. $\lambda=0.99$ is also reasonable since the fast destroy and repair function allows for many iterations and preferring a function can have long term effects.

\begin{table}[H]
\centering
\centerline{\begin{tabular}{rr|cccc}
 &  & \multicolumn{4}{c}{\texttt{w\_current}}\\
 &  & 1 & 3 & 5 & 10\\
\hline
\multirow{3}{*}{\texttt{w\_global}} & 5 & (0.45, 0.15) & (0.55, 0.14) & (0.67, 0.04) & (0.39, 0.09)\\
 & 10 & (0.42, 0.12) & (0.58, 0.01) & (0.55, 0.06) & (0.35, 0.06)\\
 & 20 & (0.52, 0.18) & (0.40, 0.11) & (0.43, 0.11) & (0.58, 0.10)\\
\end{tabular}}
\caption{Shows $(\mu, \sigma)$ with \texttt{update\_lambda=0.99} and \texttt{remove=1} fixed}
\end{table}

It is a bit strange that $w_{global}$ and $w_{current}$ should have the same value. However it is likely that a globally better solution is not reached because of one good choice of a repair and destroy method, but instead a long line of good choices. The at the time chosen repair and destroy functions are thus not a direct contributing factor to the globally better solution.

\begin{table}[H]
\centering
\begin{tabular}{r|cc}
parameter & search space & value \\ \hline
$\lambda$ & $\{0.9, 0.95, 0.99\}$ & 0.99 \\
$w_{global}$ & $\{5, 10, 20\}$ & 10 \\
$w_{current}$ & $\{1, 3, 4, 10\}$ & 10 \\
remove & $\{1, 3, 5\}$ & 1 \\
\end{tabular}
\caption{Best ALNS parameters with $\mu = 0.3502$ and $\sigma = 0.0594$}
\end{table}